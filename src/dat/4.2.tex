One of the main goals of this work has been to implement the algorithm mentioned in the previous section, to evaluate its correctness in several test simulations and finally, to exploit resulting implementation for simulations of tightly focused Gaussian beams in laser-matter interaction. The main requirement on implementation was fast integration with the 2D version of particle-in-cell simulation code EPOCH \cite{bennett}. For this reason, several possible solutions have been taken into account.

The final decision was to create a static library, which will be able to compute desired quantities and will provide functions for communication with the main simulation code. The essential advantage is that it could be basically linked with any laser-plasma simulation code. Also, since it is necessary to call only two additional functions, the instrumentation will be fast, easy and the main simulation code will not be excessively disturbed. Furthermore, the implementation itself come with the CMake \cite{Cmake2012} support, which simplify the compilation process using platform and compiler independent configuration files.

The library has been written in C++ language in the object-oriented style, so the algorithm can be easily extended to the three dimensional geometry. In order to speedup the whole underlying computation, the algorithm has been parallelized using hybrid techniques. The time domain has been decomposed into the stripes corresponding to individual computational processes, the communication between these processes is ensured by MPI library \cite{MPI1994}. Furthermore, the computationally most expensive cycles are parallelized using OpenMP implementation of multi-threading \cite{OpenMP1998}. Later on, the speedup and parallel scaling performance will be briefly discussed.

Fourier transforms form the core of the computational process and their performance is crucial for the overall performance of the code. For this reason, many currently available libraries have been considered. Eventually, the Fourier transforms in the algorithm can be computed using FFTW \cite{Frigo1998} library, Intel$ ^{\scriptsize \textregistered} $ MKL \cite{MKL2009} library or it is also possible to directly evaluate the formulas without using any additional library. User specifies his option before the compilation of the code. Regarding both libraries, a threaded versions of 1D in-place complex fast Fourier transforms have been used throughout the code. According to several measures, there is no significant difference between the speed of both libraries. On Intel$ ^{\scriptsize \textregistered} $ architectures, the MKL library \cite{MKL2009} has slightly better performance though.

One potential bottleneck could happen during the computation of spatial Fourier transforms since the arrays with spatial data are decomposed into different processes. The cluster versions of functions performing the Fourier transforms have been tested, however they did not bring any significant speedup. The reason is as follows. They require to have the global array in memory and use its own distribution which involves overlapping. Since the size of global arrays is usually not so large and since it is necessary to perform a lot of different Fourier transforms, the majority of computational time is spent rather for communication, mainly if many of computational cores are used.

This issue has been solved by gathering the data on master process, performing the Fourier transforms in space by only one processor and scattering the data back to corresponding processes. This is the reason why the code does not scale well, however, the time to compute all desired quantities is in most cases negligible in comparison with the time required by the main simulation cycle. Nevertheless, this could be a suggestion for future improvement.

Since it is necessary to compute the whole time evolution of the laser field at boundary for each grid point before the simulation starts, the resulting amount of data can be significantly large and does not have to fit in a computer RAM. Thus, it is inevitable to dump the data into a file, which will be then accessed by the main simulation code. Due to the performance purposes, each computational process stores its data into a shared file with corresponding offset and in binary coding. Therefore, the output operations are as fast as possible and save the storage resources. Library then provide a function which allows to seek an arbitrary position in a file. This function is then called each time step of the main simulation loop to fill the laser source arrays with all the relevant data. This way of accessing data does not cause any significant slowdown or memory overhead.

The EPOCH \cite{bennett} code require only transverse components of the laser electric field, all other quantities are computed by the FDTD solver \cite{ruhl}. The implementation of the library allows full connection with EPOCH \cite{bennett}. In practice, if user wants to simulate tight-focusing, it is necessary to enable the corresponding flag as a compile-time option and then to specify all required parameters in the input file. The code then automatically computes all necessary data. It works generally regardless the number of lasers in the simulation or boundaries that they are attached to.

The current version of library does not work for obliquely incident laser pulses, because in this case one cannot exploit the advantage of an efficient computation with fast Fourier transforms. However, the code allows to compute the laser fields at boundary by evaluating Fourier integrals directly, so it could be easily extended. Second, it is at the moment possible to simulate only Gaussian laser pulses. However, user can prescribe its own shape and position of the beam in focal plane by modifying corresponding part of the code.

Several most important data structures, functions and methods that form the core of the library for tight-focusing can be seen in appendix B.